{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "# import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mongo\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import multiprocess as mp\n",
    "# import mongo\n",
    "from cgcnn.data import StructureData, ListDataset, StructureDataTransformer\n",
    "import tqdm\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from skorch.callbacks import Checkpoint, LoadInitState \n",
    "from cgcnn.data import collate_pool, MergeDataset\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "from skorch import NeuralNetRegressor\n",
    "import skorch.callbacks.base\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "from adamwr.adamw import AdamW\n",
    "from adamwr.cosine_scheduler import CosineLRWithRestarts\n",
    "\n",
    "def round_(n, decimals=0):\n",
    "    '''\n",
    "    Python can't round for jack. We use someone else's home-brew to do\n",
    "    rounding. Credit goes to David Amos\n",
    "    (<https://realpython.com/python-rounding/#rounding-half-up>).\n",
    "    '''\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.floor(n*multiplier + 0.5) / multiplier\n",
    "\n",
    "def get_surface_from_doc(doc):\n",
    "    '''\n",
    "    Some of our functions parse by \"surface\", which we identify by mpid, Miller\n",
    "    index, shift, and whether it's on the top or bottom of the slab. This\n",
    "    helper function parses an aggregated/projected Mongo document for you and\n",
    "    gives you back a tuple that contains these surface identifiers.\n",
    "\n",
    "    Arg:\n",
    "        doc     A Mongo document (dictionary) that contains the keys 'mpid',\n",
    "                'miller', 'shift', and 'top'.\n",
    "    Returns:\n",
    "        surface A 4-tuple whose elements are the mpid, Miller index, shift, and\n",
    "                a Boolean indicating whether the surface is on the top or\n",
    "                bottom of the slab. Note that the Miller indices will be\n",
    "                formatted as a string, and the shift will be rounded to 2\n",
    "                decimal places.\n",
    "    '''\n",
    "    surface = (doc['mpid'], str(doc['miller']), round_(doc['shift'], 2), doc['top'])\n",
    "    return surface\n",
    "\n",
    "\n",
    "def get_docs_file(dataset, num_docs):\n",
    "    start = time.time()\n",
    "#    print('Shyam get_docs_file 1')\n",
    "    docs_all = pickle.load(open(dataset, 'rb'))\n",
    "#    print('Shyam get_docs_file 2')\n",
    "    total_docs = len(docs_all)\n",
    "    for doc in docs_all:\n",
    "        doc['surface'] = get_surface_from_doc(doc)\n",
    "#    print('Shyam get_docs_file 3')\n",
    "    docs = docs_all[:num_docs]\n",
    "#    print('Shyam get_docs_file 4')\n",
    "    target_list = np.array([doc['energy'] for doc in docs]).reshape(-1,1)\n",
    "#    target_file = open('./input/target_list.pkl','wb')\n",
    "  #  pickle.dump(target_list, target_file)\n",
    "#    print('Shyam get_docs_file 5')\n",
    "    end = time.time()\n",
    "    Docs_time = end - start\n",
    "    print('Documents loaded')\n",
    "    return docs, Docs_time, total_docs\n",
    "\n",
    "## Shyam's function ##\n",
    "def make_SDT_list(dataset):\n",
    "    SDT = StructureDataTransformer(atom_init_loc='./input/atom_init.json',\n",
    "                                  max_num_nbr=12,\n",
    "                                   step=0.2,\n",
    "                                  radius=1,\n",
    "                                  use_tag=False,\n",
    "                                  use_fixed_info=False,\n",
    "                                  use_distance=True,\n",
    "                                  train_geometry = 'final-adsorbate'\n",
    "                                  )\n",
    "\n",
    "    SDT_out = SDT.transform(dataset)\n",
    "\n",
    "    structures = SDT_out[0]\n",
    "\n",
    "    #Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "    orig_atom_fea_len = structures[0].shape[-1]\n",
    "    nbr_fea_len = structures[1].shape[-1]\n",
    "    SDT_out = SDT.transform(dataset)\n",
    "#    print(len(SDT_out))\n",
    "#    SDT_lists = list()\n",
    "    with mp.Pool(4) as pool:\n",
    "        SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))\n",
    "\n",
    "    with open('./input/SDT_list.pkl','wb') as file_out:\n",
    "        pickle.dump(SDT_list, file_out)\n",
    "        \n",
    "    print('Shyam make SDT list pickle file created')\n",
    "\n",
    "\n",
    "def get_SDT_list(dataset):\n",
    "    start = time.time()\n",
    "    SDT = StructureDataTransformer(atom_init_loc='./input/atom_init.json',\n",
    "                                  max_num_nbr=12,\n",
    "                                   step=0.2,\n",
    "                                  radius=1,\n",
    "                                  use_tag=False,\n",
    "                                  use_fixed_info=False,\n",
    "                                  use_distance=True,\n",
    "                                  train_geometry = 'final-adsorbate'\n",
    "                                  )\n",
    "\n",
    "    SDT_out = SDT.transform(dataset)\n",
    "\n",
    "    structures = SDT_out[0]\n",
    "\n",
    "    #Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "    orig_atom_fea_len = structures[0].shape[-1]\n",
    "    nbr_fea_len = structures[1].shape[-1]\n",
    "    SDT_out = SDT.transform(dataset)\n",
    "\n",
    "    with mp.Pool(4) as pool:\n",
    "        SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))\n",
    "        \n",
    "    end = time.time()\n",
    "    SDT_time = end-start\n",
    "    print('SDT list created')\n",
    "    return SDT_time\n",
    "\n",
    "def get_device():\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device='cpu'\n",
    "\n",
    "    return device\n",
    "\n",
    "def shuffle(SDT_list, target_list):\n",
    "    indices = np.arange(len(SDT_list))\n",
    "#    indices = len(SDT_list)\n",
    "#    indices = np.arange(SDT_list)\n",
    "#    print('Shyam shuffle 1')\n",
    "    print(indices)\n",
    "    SDT_training, SDT_test, target_training, target_test, train_idx, test_idx = \\\n",
    "    train_test_split(SDT_list, target_list,indices, test_size=0.2, random_state=42)\n",
    "    print('Shyam shuffle end')\n",
    "    return SDT_training, SDT_test, target_training, target_test\n",
    "\n",
    "def training(device, num_epochs, SDT_training, target_training):\n",
    "    structures = SDT_training[0]\n",
    "    print('Shyam %s'%type(structures))\n",
    "    orig_atom_fea_len = structures[0].shape[-1]\n",
    "    nbr_fea_len = structures[1].shape[-1]\n",
    "    print('Shyam training 1')\n",
    "#   train_test_splitter = ShuffleSplit(test_size=0.25, random_state=42)\n",
    "    train_test_splitter = ShuffleSplit(n_splits= 1, test_size=0.25, train_size = 0.25)\n",
    "\n",
    "    \n",
    "    # warm restart scheduling from https://arxiv.org/pdf/1711.05101.pdf\n",
    "#    LR_schedule = LRScheduler(CosineLRWithRestarts, batch_size=214, epoch_size=len(SDT_training), \\\n",
    "#                              restart_period=10, t_mult=1.2)    \n",
    "    LR_schedule = LRScheduler(CosineLRWithRestarts, batch_size=10, epoch_size=len(SDT_training), \\\n",
    "                              restart_period=10, t_mult=1.2)\n",
    "    \n",
    "    #Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "    cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "    #Callback to load the checkpoint with the best validation loss at the end of training\n",
    "    class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "        def on_train_end(self, net, X, y):\n",
    "            net.load_params('valid_best_params.pt')\n",
    "    load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "    \n",
    "    print('Shyam training 2')\n",
    "    \n",
    "    # To extract intermediate features, set the forward takes only the first return value to calculate loss\n",
    "    class MyNet(NeuralNetRegressor):\n",
    "        def get_loss(self, y_pred, y_true, **kwargs):\n",
    "            y_pred = y_pred[0] if isinstance(y_pred, tuple) else y_pred  # discard the 2nd output\n",
    "            return super().get_loss(y_pred, y_true, **kwargs)\n",
    "    print('Shyam training 3')    \n",
    "    \n",
    "    '''\n",
    "    net = MyNet(\n",
    "        CrystalGraphConvNet,\n",
    "        module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "        module__nbr_fea_len = nbr_fea_len,\n",
    "        batch_size=214,\n",
    "        module__classification=False,\n",
    "        lr=0.0056,\n",
    "        max_epochs= num_epochs, \n",
    "        module__atom_fea_len=46,\n",
    "        module__h_fea_len=83,\n",
    "        module__n_conv=8, #8\n",
    "        module__n_h=4,\n",
    "        optimizer__weight_decay=1e-5,\n",
    "        optimizer=AdamW,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_train__num_workers=0,\n",
    "        iterator_train__collate_fn = collate_pool,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        iterator_valid__num_workers=0,\n",
    "        iterator_valid__collate_fn = collate_pool,\n",
    "        device=device,\n",
    "    #     criterion=torch.nn.MSELoss,\n",
    "        criterion=torch.nn.L1Loss,\n",
    "        dataset=MergeDataset,\n",
    "        train_split = CVSplit(cv=train_test_splitter),\n",
    "        callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    "    )\n",
    "    '''\n",
    "\n",
    "#    data = MergeDataset(SDT_training, target_training)\n",
    "    data = MergeDataset\n",
    "\n",
    "    net = MyNet(\n",
    "        CrystalGraphConvNet,\n",
    "        module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "        module__nbr_fea_len = nbr_fea_len,\n",
    "        batch_size=10,\n",
    "        module__classification=False,\n",
    "        lr=0.0056,\n",
    "        max_epochs= num_epochs, \n",
    "        module__atom_fea_len=46,\n",
    "        module__h_fea_len=83,\n",
    "        module__n_conv=8, #8\n",
    "        module__n_h=4,\n",
    "        optimizer__weight_decay=1e-5,\n",
    "        optimizer=AdamW,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_train__num_workers=0,\n",
    "        iterator_train__collate_fn = collate_pool,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        iterator_valid__num_workers=0,\n",
    "        iterator_valid__collate_fn = collate_pool,\n",
    "        device=device,\n",
    "    #     criterion=torch.nn.MSELoss,\n",
    "        criterion=torch.nn.L1Loss,\n",
    "        dataset=data,\n",
    "        train_split = CVSplit(cv=train_test_splitter),\n",
    "        callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    "    )    \n",
    "\n",
    "    print('Shyam training 4')\n",
    "    \n",
    "    start = time.time()\n",
    "    net.initialize()\n",
    "    print('Shyam training 5 -- net done')\n",
    "\n",
    "#    print('Shyam training 5')\n",
    "\n",
    "    net.fit(SDT_training, target_training)\n",
    "    print('Shyam training end')\n",
    "    end = time.time()\n",
    "    training_time = end-start\n",
    "    return net, train_test_splitter, training_time\n",
    "\n",
    "\n",
    "def prediction(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs, device):\n",
    "    docs, Docs_time, total_docs = get_docs_file(dataset, num_docs)\n",
    "    SDT_time = get_SDT_list(docs)\n",
    "    print('SDT_list and target_list are loaded')\n",
    "\n",
    "#    SDT_list = pickle.load(open(SDT_list, 'rb'))\n",
    "#    target_list = pickle.load(open(target_list, 'rb'))\n",
    "\n",
    "    SDT_list = SDT_list[:num_SDT]\n",
    "    target_list = target_list[:num_SDT]\n",
    "    \n",
    " #   print('Shyam prediction 2')\n",
    "\n",
    "    docs, Docs_time, total_docs = get_docs_file(dataset, num_docs)\n",
    "    SDT_time = get_SDT_list(docs)\n",
    "    print('Shyam prediction 3')\n",
    "    \n",
    "    SDT_training, SDT_test, target_training, target_test = shuffle(SDT_list, target_list)\n",
    "    net, train_test_splitter, training_time = training(device, num_epochs, SDT_training, target_training)    \n",
    "    train_indices, valid_indices = next(train_test_splitter.split(SDT_training))\n",
    "    print('Shyam prediction 4')\n",
    "    train_error = mean_absolute_error(target_training[train_indices].reshape(-1), \n",
    "                                      net.predict(SDT_training)[train_indices].reshape(-1))\n",
    "    val_error = mean_absolute_error(target_training[valid_indices].reshape(-1), \n",
    "                                      net.predict(SDT_training)[valid_indices].reshape(-1))\n",
    "    test_error = mean_absolute_error(target_test.reshape(-1), \n",
    "                                      net.predict(SDT_test).reshape(-1))\n",
    "    start = time.time()\n",
    "    \n",
    "    measure_pred_time = mean_absolute_error(target_list[:num_docs].reshape(-1), \n",
    "                                      net.predict(SDT_list[:num_docs]).reshape(-1))\n",
    "    end = time.time()\n",
    "    pred_time = end - start\n",
    "    times = (Docs_time, SDT_time, training_time, pred_time)\n",
    "    errors = (train_error, val_error, test_error)\n",
    "    return errors, times, SDT_training, total_docs\n",
    "\n",
    "def figure_of_merit(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs):\n",
    "    device = get_device()\n",
    "    print('Shyam figure of merit 1')\n",
    "    errors, times, SDT_training, total_docs = prediction(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs, device)\n",
    "    print('Shyam figure of merit 2')\n",
    "    Docs_time, SDT_time, training_time, pred_time = times\n",
    "    print('Shyam figure of merit 3')\n",
    "    train_error, val_error, test_error = errors\n",
    "    print('\\n')   \n",
    "    print('BENCHMARK RESULTS')\n",
    "    print('Current device:', device)\n",
    "    print('Time to load %d documents: %f seconds\\n' %(total_docs, Docs_time))\n",
    "    print('Time to convert %d documents into SDT list: %f seconds' %(num_docs, SDT_time))\n",
    "    print('Time to train the model using %d training examples for %d epochs: %f seconds' %(len(SDT_training), num_epochs, training_time))\n",
    "    print('Training error: %f ev'  %train_error)\n",
    "    print('Validation error: %f ev'  %val_error)\n",
    "    print('Test error: %f ev'  %test_error)\n",
    "    print('Time to predict energy for %d documents: %f seconds' %(num_docs, pred_time))\n",
    "    with open('result.md', 'w') as f:\n",
    "        f.writelines('# Benchmark Test \\n\\n')\n",
    "        f.writelines('## Measurement on Edison\\n\\n')\n",
    "        f.writelines('```C\\n') \n",
    "        f.writelines('sbatch run_benchmark.sh \\n')\n",
    "        f.writelines('```\\n\\n')\n",
    "        f.writelines('## Benchmark Results \\n')\n",
    "        if device == 'cpu':\n",
    "            f.writelines('Current device:' + device + '\\n')\n",
    "        else:\n",
    "            f.writelines('Current device: ' + device.type + '\\n')\n",
    "        f.writelines('\\nTime to load %d documents: %f seconds\\n' %(total_docs, Docs_time))\n",
    "        f.writelines('\\nTime to convert %d documents into SDT list: %f seconds\\n' %(num_docs, SDT_time))\n",
    "        f.writelines('\\nTime to train the model using %d training examples: %f seconds\\n' %(len(SDT_training), training_time))\n",
    "        f.writelines('\\nTraining error: %f ev\\n'  %train_error)\n",
    "        f.writelines('\\nValidation error: %f ev\\n'  %val_error)\n",
    "        f.writelines('\\nTest error: %f ev\\n'  %test_error)\n",
    "        f.writelines('\\nTime to predict energy for %d documents: %f seconds\\n' %(num_docs, pred_time))\n",
    "    return\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.49401431]\n",
      " [-0.21384117]\n",
      " [-0.01293161]\n",
      " [ 0.09524271]\n",
      " [-1.03625676]\n",
      " [ 0.0541775 ]\n",
      " [ 0.00765951]\n",
      " [ 0.36665157]\n",
      " [ 0.33331164]\n",
      " [-0.61652263]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:33<00:00,  3.31s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shyam make SDT list pickle file created\n",
      "Shyam figure of merit 1\n",
      "Documents loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [00:48<00:00,  4.82s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDT list created\n",
      "SDT_list and target_list are loaded\n",
      "Documents loaded\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10/10 [01:06<00:00,  6.63s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SDT list created\n",
      "Shyam prediction 3\n",
      "[0 1 2 3 4]\n",
      "Shyam shuffle end\n",
      "Shyam <class 'str'>\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-1a427590b9a5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0;31m#num_SDT=20771\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m \u001b[0mfigure_of_merit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_SDT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-f74f5f5529b8>\u001b[0m in \u001b[0;36mfigure_of_merit\u001b[0;34m(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs)\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shyam figure of merit 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m     \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_SDT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shyam figure of merit 2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mDocs_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f74f5f5529b8>\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs, device)\u001b[0m\n\u001b[1;32m    287\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    288\u001b[0m     \u001b[0mSDT_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSDT_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 289\u001b[0;31m     \u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_test_splitter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    290\u001b[0m     \u001b[0mtrain_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalid_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_test_splitter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSDT_training\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    291\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shyam prediction 4'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-7-f74f5f5529b8>\u001b[0m in \u001b[0;36mtraining\u001b[0;34m(device, num_epochs, SDT_training, target_training)\u001b[0m\n\u001b[1;32m    163\u001b[0m     \u001b[0mstructures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSDT_training\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shyam %s'\u001b[0m\u001b[0;34m%\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstructures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 165\u001b[0;31m     \u001b[0morig_atom_fea_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    166\u001b[0m     \u001b[0mnbr_fea_len\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstructures\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    167\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Shyam training 1'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "num_docs=10\n",
    "num_SDT=5\n",
    "\n",
    "# Read dataset from file\n",
    "file = open('./input/mat_10.pkl','rb')\n",
    "datas = pickle.load(file)\n",
    "\n",
    "# Write target energies to file\n",
    "target_array = np.array([data['energy'] for data in datas]).reshape(-1, 1)\n",
    "print(target_array)\n",
    "with open('./input/target_list.pkl','wb') as target_file:\n",
    "    pickle.dump(target_array, target_file)\n",
    "\n",
    "# Write SDT file\n",
    "make_SDT_list(datas)\n",
    "\n",
    "#b = get_docs_file(dataset, num_docs)\n",
    "\n",
    "\n",
    "dataset='./input/mat_10.pkl'\n",
    "SDT_list='./input/SDT_list.pkl'\n",
    "target_list='./input/target_list.pkl'\n",
    "\n",
    "#num_SDT=20771\n",
    "num_epochs=2\n",
    "figure_of_merit(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
