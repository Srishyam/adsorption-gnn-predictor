{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "\n",
    "import sys\n",
    "# import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mongo\n",
    "import time\n",
    "import pickle\n",
    "import math\n",
    "import torch\n",
    "from torch.optim import Adam, SGD\n",
    "# from torch.utils.data import Dataset, DataLoader\n",
    "import multiprocess as mp\n",
    "# import mongo\n",
    "from cgcnn.data import StructureData, ListDataset, StructureDataTransformer\n",
    "import tqdm\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "from skorch.callbacks import Checkpoint, LoadInitState \n",
    "from cgcnn.data import collate_pool, MergeDataset\n",
    "from cgcnn.model import CrystalGraphConvNet\n",
    "from skorch import NeuralNetRegressor\n",
    "import skorch.callbacks.base\n",
    "from skorch.dataset import CVSplit\n",
    "from skorch.callbacks.lr_scheduler import WarmRestartLR, LRScheduler\n",
    "from adamwr.adamw import AdamW\n",
    "from adamwr.cosine_scheduler import CosineLRWithRestarts\n",
    "\n",
    "def round_(n, decimals=0):\n",
    "    '''\n",
    "    Python can't round for jack. We use someone else's home-brew to do\n",
    "    rounding. Credit goes to David Amos\n",
    "    (<https://realpython.com/python-rounding/#rounding-half-up>).\n",
    "    '''\n",
    "    multiplier = 10 ** decimals\n",
    "    return math.floor(n*multiplier + 0.5) / multiplier\n",
    "\n",
    "def get_surface_from_doc(doc):\n",
    "    '''\n",
    "    Some of our functions parse by \"surface\", which we identify by mpid, Miller\n",
    "    index, shift, and whether it's on the top or bottom of the slab. This\n",
    "    helper function parses an aggregated/projected Mongo document for you and\n",
    "    gives you back a tuple that contains these surface identifiers.\n",
    "\n",
    "    Arg:\n",
    "        doc     A Mongo document (dictionary) that contains the keys 'mpid',\n",
    "                'miller', 'shift', and 'top'.\n",
    "    Returns:\n",
    "        surface A 4-tuple whose elements are the mpid, Miller index, shift, and\n",
    "                a Boolean indicating whether the surface is on the top or\n",
    "                bottom of the slab. Note that the Miller indices will be\n",
    "                formatted as a string, and the shift will be rounded to 2\n",
    "                decimal places.\n",
    "    '''\n",
    "    surface = (doc['mpid'], str(doc['miller']), round_(doc['shift'], 2), doc['top'])\n",
    "    return surface\n",
    "\n",
    "\n",
    "def get_docs_file(dataset, num_docs):\n",
    "    start = time.time()\n",
    "    docs_all = pickle.load(open(dataset, 'rb'))\n",
    "    total_docs = len(docs_all)\n",
    "    for doc in docs_all:\n",
    "        doc['surface'] = get_surface_from_doc(doc)\n",
    "    docs = docs_all[:num_docs]\n",
    "    target_list = np.array([doc['energy'] for doc in docs]).reshape(-1,1)\n",
    "    end = time.time()\n",
    "    Docs_time = end - start\n",
    "    print('Documents loaded')\n",
    "    return docs, Docs_time, total_docs\n",
    "\n",
    "\n",
    "def get_SDT_list(dataset):\n",
    "    start = time.time()\n",
    "\n",
    "    SDT = StructureDataTransformer(atom_init_loc='./input/atom_init.json',\n",
    "                                  max_num_nbr=12,\n",
    "                                   step=0.2,\n",
    "                                  radius=1,\n",
    "                                  use_tag=False,\n",
    "                                  use_fixed_info=False,\n",
    "                                  use_distance=True,\n",
    "                                  train_geometry = 'final-adsorbate'\n",
    "                                  )\n",
    "\n",
    "    SDT_out = SDT.transform(dataset)\n",
    "\n",
    "    structures = SDT_out[0]\n",
    "\n",
    "    #Settings necessary to build the model (since they are size of vectors as inputs)\n",
    "    orig_atom_fea_len = structures[0].shape[-1]\n",
    "    nbr_fea_len = structures[1].shape[-1]\n",
    "    SDT_out = SDT.transform(dataset)\n",
    "    with mp.Pool(4) as pool:\n",
    "        SDT_list = list(tqdm.tqdm(pool.imap(lambda x: SDT_out[x],range(len(SDT_out)),chunksize=40),total=len(SDT_out)))\n",
    "    \n",
    "    end = time.time()\n",
    "    SDT_time = end-start\n",
    "    print('SDT list created')\n",
    "    return SDT_time\n",
    "\n",
    "def get_device():\n",
    "    cuda = torch.cuda.is_available()\n",
    "    if cuda:\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device='cpu'\n",
    "\n",
    "    return device\n",
    "\n",
    "def shuffle(SDT_list, target_list):\n",
    "    indices = np.arange(len(SDT_list))\n",
    "    SDT_training, SDT_test, target_training, target_test, train_idx, test_idx = \\\n",
    "    train_test_split(SDT_list, target_list,indices, test_size=0.2, random_state=42)\n",
    "    return SDT_training, SDT_test, target_training, target_test\n",
    "\n",
    "def training(device, num_epochs, SDT_training, target_training):\n",
    "    structures = SDT_training[0]\n",
    "    orig_atom_fea_len = structures[0].shape[-1]\n",
    "    nbr_fea_len = structures[1].shape[-1]\n",
    "    \n",
    "    train_test_splitter = ShuffleSplit(test_size=0.25, random_state=42)\n",
    "    # warm restart scheduling from https://arxiv.org/pdf/1711.05101.pdf\n",
    "    LR_schedule = LRScheduler(CosineLRWithRestarts, batch_size=214, epoch_size=len(SDT_training), \\\n",
    "                              restart_period=10, t_mult=1.2)\n",
    "    \n",
    "    #Make a checkpoint to save parameters every time there is a new best for validation lost\n",
    "    cp = Checkpoint(monitor='valid_loss_best',fn_prefix='valid_best_')\n",
    "    #Callback to load the checkpoint with the best validation loss at the end of training\n",
    "    class train_end_load_best_valid_loss(skorch.callbacks.base.Callback):\n",
    "        def on_train_end(self, net, X, y):\n",
    "            net.load_params('valid_best_params.pt')\n",
    "    load_best_valid_loss = train_end_load_best_valid_loss()\n",
    "    # To extract intermediate features, set the forward takes only the first return value to calculate loss\n",
    "    class MyNet(NeuralNetRegressor):\n",
    "        def get_loss(self, y_pred, y_true, **kwargs):\n",
    "            y_pred = y_pred[0] if isinstance(y_pred, tuple) else y_pred  # discard the 2nd output\n",
    "            return super().get_loss(y_pred, y_true, **kwargs)\n",
    "    net = MyNet(\n",
    "        CrystalGraphConvNet,\n",
    "        module__orig_atom_fea_len = orig_atom_fea_len,\n",
    "        module__nbr_fea_len = nbr_fea_len,\n",
    "        batch_size=214,\n",
    "        module__classification=False,\n",
    "        lr=0.0056,\n",
    "        max_epochs= num_epochs, \n",
    "        module__atom_fea_len=46,\n",
    "        module__h_fea_len=83,\n",
    "        module__n_conv=8, #8\n",
    "        module__n_h=4,\n",
    "        optimizer__weight_decay=1e-5,\n",
    "        optimizer=AdamW,\n",
    "        iterator_train__pin_memory=True,\n",
    "        iterator_train__num_workers=0,\n",
    "        iterator_train__collate_fn = collate_pool,\n",
    "        iterator_valid__pin_memory=True,\n",
    "        iterator_valid__num_workers=0,\n",
    "        iterator_valid__collate_fn = collate_pool,\n",
    "        device=device,\n",
    "    #     criterion=torch.nn.MSELoss,\n",
    "        criterion=torch.nn.L1Loss,\n",
    "        dataset=MergeDataset,\n",
    "        train_split = CVSplit(cv=train_test_splitter),\n",
    "        callbacks=[cp, load_best_valid_loss, LR_schedule]\n",
    "    )\n",
    "    start = time.time()\n",
    "    net.initialize()\n",
    "    net.fit(SDT_training, target_training)\n",
    "    end = time.time()\n",
    "    training_time = end-start\n",
    "    return net, train_test_splitter, training_time\n",
    "\n",
    "\n",
    "def prediction(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs, device):\n",
    "    docs, Docs_time, total_docs = get_docs_file(dataset, num_docs)\n",
    "    SDT_time = get_SDT_list(docs)\n",
    "    SDT_list = pickle.load(open(SDT_list, 'rb'))\n",
    "    target_list = pickle.load(open(target_list, 'rb'))\n",
    "    print('SDT_list and target_list are loaded')\n",
    "    SDT_list = SDT_list[:num_SDT]\n",
    "    target_list = target_list[:num_SDT]\n",
    "    \n",
    "#   docs, Docs_time, total_docs = get_docs_file(dataset, num_docs)\n",
    "#    SDT_time = get_SDT_list(docs)\n",
    "    \n",
    "    SDT_training, SDT_test, target_training, target_test = shuffle(SDT_list, target_list)\n",
    "    net, train_test_splitter, training_time = training(device, num_epochs, SDT_training, target_training)    \n",
    "    train_indices, valid_indices = next(train_test_splitter.split(SDT_training))\n",
    "    train_error = mean_absolute_error(target_training[train_indices].reshape(-1), \n",
    "                                      net.predict(SDT_training)[train_indices].reshape(-1))\n",
    "    val_error = mean_absolute_error(target_training[valid_indices].reshape(-1), \n",
    "                                      net.predict(SDT_training)[valid_indices].reshape(-1))\n",
    "    test_error = mean_absolute_error(target_test.reshape(-1), \n",
    "                                      net.predict(SDT_test).reshape(-1))\n",
    "    start = time.time()\n",
    "    \n",
    "    measure_pred_time = mean_absolute_error(target_list[:num_docs].reshape(-1), \n",
    "                                      net.predict(SDT_list[:num_docs]).reshape(-1))\n",
    "    end = time.time()\n",
    "    pred_time = end - start\n",
    "    times = (Docs_time, SDT_time, training_time, pred_time)\n",
    "    errors = (train_error, val_error, test_error)\n",
    "    return errors, times, SDT_training, total_docs\n",
    "\n",
    "def figure_of_merit(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs):\n",
    "    device = get_device()\n",
    "    errors, times, SDT_training, total_docs = prediction(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs, device)\n",
    "    Docs_time, SDT_time, training_time, pred_time = times\n",
    "    train_error, val_error, test_error = errors\n",
    "    print('\\n')   \n",
    "    print('BENCHMARK RESULTS')\n",
    "    print('Current device:', device)\n",
    "    print('Time to load %d documents: %f seconds\\n' %(total_docs, Docs_time))\n",
    "    print('Time to convert %d documents into SDT list: %f seconds' %(num_docs, SDT_time))\n",
    "    print('Time to train the model using %d training examples for %d epochs: %f seconds' %(len(SDT_training), num_epochs, training_time))\n",
    "    print('Training error: %f ev'  %train_error)\n",
    "    print('Validation error: %f ev'  %val_error)\n",
    "    print('Test error: %f ev'  %test_error)\n",
    "    print('Time to predict energy for %d documents: %f seconds' %(num_docs, pred_time))\n",
    "    with open('result.md', 'w') as f:\n",
    "        f.writelines('# Benchmark Test \\n\\n')\n",
    "        f.writelines('## Measurement on Edison\\n\\n')\n",
    "        f.writelines('```C\\n') \n",
    "        f.writelines('sbatch run_benchmark.sh \\n')\n",
    "        f.writelines('```\\n\\n')\n",
    "        f.writelines('## Benchmark Results \\n')\n",
    "        if device == 'cpu':\n",
    "            f.writelines('Current device:' + device + '\\n')\n",
    "        else:\n",
    "            f.writelines('Current device: ' + device.type + '\\n')\n",
    "        f.writelines('\\nTime to load %d documents: %f seconds\\n' %(total_docs, Docs_time))\n",
    "        f.writelines('\\nTime to convert %d documents into SDT list: %f seconds\\n' %(num_docs, SDT_time))\n",
    "        f.writelines('\\nTime to train the model using %d training examples: %f seconds\\n' %(len(SDT_training), training_time))\n",
    "        f.writelines('\\nTraining error: %f ev\\n'  %train_error)\n",
    "        f.writelines('\\nValidation error: %f ev\\n'  %val_error)\n",
    "        f.writelines('\\nTest error: %f ev\\n'  %test_error)\n",
    "        f.writelines('\\nTime to predict energy for %d documents: %f seconds\\n' %(num_docs, pred_time))\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'docs' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-520a0d05159e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mnum_SDT\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m20771\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mfigure_of_merit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_SDT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-1-51e73886ecd5>\u001b[0m in \u001b[0;36mfigure_of_merit\u001b[0;34m(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs)\u001b[0m\n\u001b[1;32m    207\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfigure_of_merit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_SDT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m     \u001b[0mdevice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m     \u001b[0merrors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_training\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtotal_docs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_SDT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    210\u001b[0m     \u001b[0mDocs_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtraining_time\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpred_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimes\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m     \u001b[0mtrain_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-51e73886ecd5>\u001b[0m in \u001b[0;36mprediction\u001b[0;34m(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs, device)\u001b[0m\n\u001b[1;32m    176\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mprediction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mSDT_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_docs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_SDT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 178\u001b[0;31m     \u001b[0mSDT_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_SDT_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    179\u001b[0m     \u001b[0mSDT_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mSDT_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m     \u001b[0mtarget_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'docs' referenced before assignment"
     ]
    }
   ],
   "source": [
    "dataset='./input/docs.pkl'\n",
    "SDT_list='./input/SDT_list.pkl'\n",
    "target_list='./input/target_list.pkl'\n",
    "num_docs=100\n",
    "num_SDT=20771\n",
    "num_epochs=10\n",
    "figure_of_merit(dataset, SDT_list, target_list, num_docs, num_SDT, num_epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
